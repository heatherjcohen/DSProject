{
 "metadata": {
  "name": "",
  "signature": "sha256:fcd0526280ff79b491a700c207be5492e2d511417f581469aadf3ae4d18e85a3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "import tweepy\n",
      "import json\n",
      "from pymongo import MongoClient\n",
      "from bson.objectid import ObjectId\n",
      "from sklearn import cross_validation\n",
      "client = MongoClient('localhost',27017)\n",
      "\n",
      "db = client.dump\n",
      "print db.collection_names()\n",
      "count = db.tweets.count()\n",
      "print db.tweets.find({'fem':1}).count() #neg\n",
      "print db.tweets.find({'fem':0}).count() #pos\n",
      "print db.tweets.find({\"fem\": {\"$exists\": False}}).count() #test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'langs', u'system.indexes', u'tweets']\n",
        "217\n",
        "217\n",
        "30163"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neg_tweets = [(doc['text']) for doc in db.tweets.find({'fem':1})] \n",
      "pos_tweets = [(doc['text']) for doc in db.tweets.find({'fem':0})] \n",
      "neg_sent =  [(doc['fem']) for doc in db.tweets.find({'fem':1})] \n",
      "pos_sent = [(doc['fem']) for doc in db.tweets.find({'fem':0})] \n",
      "all_tweets= [(doc['text']) for doc in db.tweets.find({\"text\": {\"$exists\":True}})] \n",
      "traintest_tweets = pos_tweets + neg_tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = neg_sent + pos_sent\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vect = CountVectorizer(analyzer='char_wb', ngram_range=(2, 5), encoding=u'utf-8', stop_words='english')\n",
      "ngvText = vect.fit_transform(pos_tweets + neg_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_tweets =[(doc['text']) for doc in db.tweets.find({\"fem\": {\"$exists\": False}})]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(ngvText, sent, random_state=0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pred_tweets = vect.transform(new_tweets)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "nb = MultinomialNB()\n",
      "nb.fit(X_train, y_train)\n",
      "print nb.classes_\n",
      "print nb.score(X_test, y_test)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1]\n",
        "0.642201834862\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from __future__ import division\n",
      "predsMNB = nb.predict(pred_tweets)\n",
      "nonzero = np.count_nonzero(predsMNB)\n",
      "print nonzero/count #percent negative\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score \n",
      "scores = cross_val_score(nb, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.576853939929\n",
        "Accuracy: 0.52 (+/- 0.00)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "clf = BernoulliNB()\n",
      "clf.fit(X_train, y_train)\n",
      "print clf.score(X_test, y_test)\n",
      "scores = cross_val_score(clf, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.623853211009\n",
        "Accuracy: 0.53 (+/- 0.00)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predsBNB= clf.predict(pred_tweets)\n",
      "nonzero2 = np.count_nonzero(predsBNB)\n",
      "print nonzero2/count #percent negative"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.488806092101\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
      "from sklearn.linear_model import LogisticRegression \n",
      "logreg = LogisticRegression()\n",
      "logreg = logreg.fit(X_train, y_train)\n",
      "predlog = logreg.predict(pred_tweets)\n",
      "nonzero3 = np.count_nonzero(predlog)\n",
      "print nonzero3/count #percent negative\n",
      "print logreg.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.534529529039\n",
        "0.577981651376\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(logreg, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.55 (+/- 0.00)\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import svm\n",
      "\n",
      "transformer = TfidfTransformer()\n",
      "#turns into sparse array\n",
      "X_train_tfidf = transformer.fit_transform(X_train)\n",
      "X_test_tfidf = transformer.fit_transform(X_test)\n",
      "X_new_data = transformer.fit_transform(pred_tweets)\n",
      "\n",
      "svm = LinearSVC()\n",
      "svm.fit(X_train_tfidf, y_train)\n",
      "predsLSVC = svm.predict(X_new_data)\n",
      "nonzero4 = np.count_nonzero(predsLSVC)\n",
      "print nonzero4/count #percent negative\n",
      "\n",
      "\n",
      "scores = cross_val_score(svm, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.519789521849\n",
        "Accuracy: 0.58 (+/- 0.00)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "nb2 = MultinomialNB()\n",
      "nb2.fit(X_train_tfidf, y_train)\n",
      "predsMNBT = nb2.predict(X_new_data)\n",
      "scores = cross_val_score(nb2, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.52 (+/- 0.00)\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "clf2 = BernoulliNB()\n",
      "clf2.fit(X_train_tfidf, y_train)\n",
      "predsBNBT = clf2.predict(X_new_data)\n",
      "scores = cross_val_score(clf2, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.53 (+/- 0.00)\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
      "from sklearn.linear_model import LogisticRegression \n",
      "logreg2 = LogisticRegression()\n",
      "logreg2 = logreg2.fit(X_train_tfidf, y_train)\n",
      "predlog2 = logreg2.predict(X_new_data)\n",
      "scores = cross_val_score(logreg2, X_test, y_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.55 (+/- 0.00)\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n",
      "\n",
      "vectorizer = TfidfVectorizer(min_df=2, \n",
      " ngram_range=(1, 2), \n",
      " stop_words='english', \n",
      " strip_accents='unicode', \n",
      " lowercase = False,\n",
      " norm='l2')\n",
      "\n",
      "test_string = unicode(all_tweets[0])\n",
      "\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: RT @punkthetic: Feminist support gender equality not female supremacy get your facts straight before you post something ignorant on the Int\u2026\n",
        "Preprocessed string: RT @punkthetic: Feminist support gender equality not female supremacy get your facts straight before you post something ignorant on the Int...\n",
        "Tokenized string:[u'RT', u'punkthetic', u'Feminist', u'support', u'gender', u'equality', u'not', u'female', u'supremacy', u'get', u'your', u'facts', u'straight', u'before', u'you', u'post', u'something', u'ignorant', u'on', u'the', u'Int']\n",
        "N-gram data string:[u'RT', u'punkthetic', u'Feminist', u'support', u'gender', u'equality', u'female', u'supremacy', u'facts', u'straight', u'post', u'ignorant', u'Int', u'RT punkthetic', u'punkthetic Feminist', u'Feminist support', u'support gender', u'gender equality', u'equality female', u'female supremacy', u'supremacy facts', u'facts straight', u'straight post', u'post ignorant', u'ignorant Int']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2_train, X2_test, y2_train, y2_test = cross_validation.train_test_split(traintest_tweets, sent)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2_train = vectorizer.fit_transform(X2_train)\n",
      "X2_test = vectorizer.transform(X2_test)\n",
      "\n",
      "nb_classifier = MultinomialNB().fit(X2_train, y2_train)\n",
      "\n",
      "y_nb_predicted = nb_classifier.predict(X2_test)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y2_test, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y2_test, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y2_test, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y2_test, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y2_test, y_nb_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "scores = cross_val_score(nb_classifier, X2_test, y2_test, cv=10).mean()\n",
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "\n",
        "The precision for this classifier is 0.609375\n",
        "The recall for this classifier is 0.709090909091\n",
        "The f1 for this classifier is 0.655462184874\n",
        "The accuracy for this classifier is 0.623853211009\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.64      0.54      0.59        54\n",
        "          1       0.61      0.71      0.66        55\n",
        "\n",
        "avg / total       0.63      0.62      0.62       109\n",
        "\n",
        "Accuracy: 0.51 (+/- 0.00)\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}