{
 "metadata": {
  "name": "",
  "signature": "sha256:539ca211880f627350f4d0aa8d9d2561be16dac41a32aa863447ec763bb8a504"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "import tweepy\n",
      "import json\n",
      "from pymongo import MongoClient\n",
      "from bson.objectid import ObjectId\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import metrics\n",
      "from sklearn import cross_validation\n",
      "\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "\n",
      "from sklearn.preprocessing import Normalizer\n",
      "\n",
      "client = MongoClient('localhost',27017)\n",
      "\n",
      "db = client.dump"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.collection_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "[u'langs', u'system.indexes', u'tweets']"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neg_tweets = [(doc['text']) for doc in db.tweets.find({'fem':1})] \n",
      "pos_tweets = [(doc['text']) for doc in db.tweets.find({'fem':0})] \n",
      "neg_sent =  [(doc['fem']) for doc in db.tweets.find({'fem':1})] \n",
      "pos_sent = [(doc['fem']) for doc in db.tweets.find({'fem':0})] \n",
      "sent = neg_sent + pos_sent\n",
      "all_tweets= [(doc['text']) for doc in db.tweets.find({\"text\": {\"$exists\":True}})] \n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vect = CountVectorizer(analyzer='char_wb', ngram_range=(2, 5), encoding=u'utf-8', stop_words='english', max_features=10000)\n",
      "ngvText = vect.fit_transform(all_tweets)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "import gensim \n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from operator import itemgetter\n",
      "import re\n",
      "\n",
      "\n",
      "\n",
      "documents = all_tweets\n",
      "texts = [[word for word in document.lower().split()]\n",
      " for document in documents]\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named gensim",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-132-8ba6b48fcc74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named gensim"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
      "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "mbk = MiniBatchKMeans()\n",
      "mbk = mbk.fit(ngvText)\n",
      "labels= mbk.labels_\n",
      "print \"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, mbk.labels_)\n",
      "print \"Completeness: %0.3f\" % metrics.completeness_score(labels, mbk.labels_)\n",
      "print \"V-measure: %0.3f\" % metrics.v_measure_score(labels, mbk.labels_)\n",
      "print \"Adjusted Rand-Index: %.3f\" % \\\n",
      "    metrics.adjusted_rand_score(labels, mbk.labels_)\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Homogeneity: 1.000\n",
        "Completeness: 1.000\n",
        "V-measure: 1.000\n",
        "Adjusted Rand-Index: 1.000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
      "transformer = TfidfTransformer()\n",
      "X_tfidf = transformer.fit_transform(ngvText)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mbk2 = MiniBatchKMeans()\n",
      "mbk2 = mbk.fit(ngvText)\n",
      "labels2= mbk2.labels_\n",
      "print \"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels2, mbk2.labels_)\n",
      "print \"Completeness: %0.3f\" % metrics.completeness_score(labels2, mbk2.labels_)\n",
      "print \"V-measure: %0.3f\" % metrics.v_measure_score(labels2, mbk2.labels_)\n",
      "print \"Adjusted Rand-Index: %.3f\" % \\\n",
      "    metrics.adjusted_rand_score(labels2, mbk2.labels_)\n",
      "print labels2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Homogeneity: 1.000\n",
        "Completeness: 1.000\n",
        "V-measure: 1.000\n",
        "Adjusted Rand-Index: 1.000\n",
        "[5 5 5 5 5 3 5 5 3 3 5 5 5 5 5 5 5 3 5 5 5 5 5 5 3 5 5 5 3 5 5 5 5 3 7 5 5\n",
        " 5 3 5 5 3 5 5 5 5 3 5 5 5 5 5 5 7 3 5 5 5 5 7 7 5 3 5 5 5 3 5 5 7 3 5 5 5\n",
        " 5 3 5 5 3 5 3 5 5 3 5 5 5 5 5 5 3 5 5 5 3 5 5 5 5 5 5 7 5 5 5 5 3 5 5 5 3\n",
        " 5 5 3 5 3 5 5 5 5 5 5 5 3 5 5 7 3 5 5 5 7 5 3 5 5 5 5 5 5 3 5 5 3 5 5 5 5\n",
        " 5 5 3 5 5 5 5 5 3 5 3 3 3 3 5 5 5 5 3 5 5 5 5 7 3 3 5 5 5 3 5 3 3 5 5 5 5\n",
        " 5 5 5 3 5 5 5 5 5 3 5 5 3 5 5 3 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 3 3 5 5 3\n",
        " 5 5 5 5 5 3 5 5 5 5 5 3 5 5 5 5 5 5 3 3 5 3 5 5 5 5 5 5 5 5 5 5 5 3 5 3 5\n",
        " 5 5 3 5 3 5 7 5 5 5 5 3 5 3 5 5 5 5 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 3 5 5 5\n",
        " 5 5 3 5 3 5 5 5 3 5 5 5 3 7 5 5 5 5 3 5 5 5 5 5 5 5 5 3 5 3 5 5 3 3 5 5 5\n",
        " 5 5 5 5 5 5 5 3 3 5 5 5 5 3 5 5 3 5 5 7 5 3 5 5 0 5 3 5 5 5 3 5 3 7 5 5 7\n",
        " 5 5 5 5 5 5 5 5 5 3 3 5 5 5 3 5 5 5 3 3 5 5 5 3 5 3 3 5 3 5 3 5 5 5 5 5 5\n",
        " 5 5 5 5 5 5 5 5 3 3 5 3 5 5 5 5 5 5 5 5 5 7 7 5 5 0 5]\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}